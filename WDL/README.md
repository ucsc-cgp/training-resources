# The WDL Library

![stock photo of dog with glasses reading from a stack of books](images/dog_with_books.png "not quite an O'Reily cover but this will do")

_Head chef: Ash O'Farrell (UC Santa Cruz)_

If you are viewing this document on GitHub, click on the ⋮≡ symbol on the upper left hand side, which will remain there as you scroll down this document. This will open a table of contents.


## Purpose and Intended Audience
WDL is a powerful workflow language, but it is specialized and relatively new. This means that most of the resources available on it are generated by developers of the WDL language itself rather than users or workflow creators. These official resources are quite helpful and serve as a useful source-of-truth, but when it comes to actually writing a workflow from scratch, it can be difficult to know where to start. Troubleshooting is likewise problematic, as the tried-and-true method of "googling the error code" is less helpful when a language has less users posting about said error code.

Dockstore is not only a repository of finished workflows, but also a general resource for successfully creating and using those workflows. In addition to this WDL-focused document, Dockstore has [comprehensive documentation](https://docs.dockstore.org/en/develop/getting-started/getting-started.html) on getting started with Docker and workflows in general. The goal of this document is not to replace that documentation but to supplement it with resources that don't quite fit the structure of traditional documentation.

This document is intended for those users who already have a basic understanding of WDL and its components, but are struggling with getting started writing their own workflow or debugging. If you are new to WDL, it may be best to start with [Dockstore's beginner WDL resources](https://docs.dockstore.org/en/develop/getting-started/getting-started-with-wdl.html) first.

This document is arranged in multiple sections with different pedalogical purposes:
* **Recipes** are large blocks of code to serve as an example or template of how to do a specific task in WDL.
* **Lessons** are explanations of a particular WDL concept which is not well-defined in the WDL spec.
* **Quick tips** are more focused on small pieces of advice and reminders.


## Contribution
To contribute to this resource, open a pull request in the GitHub repo.


## Helpful External Resources
### General WDL usage
* [Dockstore: Getting Started with WDL](https://docs.dockstore.org/en/develop/getting-started/getting-started-with-wdl.html) 
* [The WDL 1.0 Spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md)
* [miniwdl's WDL tips](https://miniwdl.readthedocs.io/en/latest/runner_advanced.html)
* [OpenWDL](https://openwdl.org/):
    * [OpenWDL GitHub Issues](https://github.com/openwdl/wdl) 


### Using WDL in the cloud
* [Terra's WDL documentation resources](https://support.terra.bio/hc/en-us/sections/360007274612-WDL-Documentation)
* [Cloud-based runtime attributes](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/)
* [Understanding and controlling cloud costs](https://support.terra.bio/hc/en-us/articles/360029748111-Understanding-and-controlling-cloud-costs-)


## Versioning, "is this document current?", etc
Unless otherwise specified, all examples in this document are using WDL 1.0 syntax.

Examples in this document were checked using version 1.12.0 of the Dockstore CLI, which uses Cromwell version 57, in July 2022. It's possible something slipped by me, so open a PR if something seems incorrect.


# Recipes
## How to get an output of an unknown filename
A challenging limitation of WDL is the fact that variables created within a task's command section cannot easily be passed out of that section. Sometimes, this isn't a problem, such as when an output file always the same filename, like here:

<!---grabbing_outputs.wdl--->
```
task consistent_output_filename {
	input {
		File gds
	}

	command <<<
		touch my_cool_file.txt
	>>>

	output {
		File out = "my_cool_file.txt"
	}
}
```

But there are scenarios where the name of your output file depends on the name of your input file. In that scenario, you can use the WDL built-in basename() function outside of the command section of your task. This will create a private variable that you can reference in your output section. (For more on variables, see [this lesson all about WDL variables](#variables-in-wdl-explanation-and-examples).)

<!---grabbing_outputs.wdl--->
```
task output_matches_input_filename {
	input {
		File gds
	}
	String gds_basename = basename(gds)

	command <<<
		touch ~{gds_basename}
	>>>

	output {
		File out = gds_basename
	}
}
```

If you don't know *anything* about the output file name except for its extension, you can use the WDL built-in glob() to return an array of files that match whatever regex pattern you specify. (I use the term regex loosely - glob() does not seem to support any regex besides the \* wildcard expression.) If only one file matches the expression, then the output will be an array with only one file in it, at index zero. Therefore, you can access the file at index zero, which makes your output variable a valid `File` instead of `Array[File]`.

<!---grabbing_outputs.wdl--->
```
task only_know_output_extension {
	input {
		File gds
	}

	command <<<
		touch $RANDOM.txt
	>>>

	output {
		File out = glob("*.txt")[0]
	}
}
```

## How to scale cloud resources with size of input files
There's no reason to request 100 GB of space from Google if you're just re-aligning a 300 MB cram file to the human genome. Instead of hardcoding a size for your tasks, it's always better to base the value on the size of one's inputs.

However, this is problematic -- `size()` returns a float, but the disk runtime attribute must be an integer, and at first glance it may appear that WDL lacks a way to directly coerce floats into integers. While you could use some clever tricks involving strings to get around this, it's far easier and less error-prone to simply round up floats into integers using `ceil()`.

Here's an example of using `ceil()` to round up the size of your inputs, then adding them together to generate a final value for disk size. The user can also specify a bigger value than 1 for `addl_disk` to add even more disk space on top. See [scaling_cloud_resources.wdl](https://github.com/ucsc-cgp/training-resources/blob/wdl-library/WDL/examples/scaling_cloud_resources.wdl) for a full WDL.

<!-- corresponds to scaling_cloud_resources.wdl-->
```
task aggregate_list {
	input {
		File some_big_file
		File some_other_file
		Int addl_disk = 1
	}

	Int big_size = ceil(size(some_big_file, "GB"))
	Int other_size = ceil(size(some_other_file, "GB"))
	Int final_disk_size = big_size + other_size + addl_disk

	command <<<
		echo "foo"
	>>>

	runtime {
		disks: "local-disk " + final_disk_size + " HDD"
	}
}
```

You'll notice `big_size`, `other_size`, and `final_disk_size` are not defined in the input section, but it exists outside the command section too. When a variable is defined in this way, it is evaluated after the input variables are evaluated, but before the command section starts. They also cannot be accessed by the user. This means that someone running this workflow on Terra will have the option to define `some_big_file`, `some_other_file`, and `addl_disk`, but not `big_size`, `other_size`, and `final_disk_size`. In other words, `big_size`, `other_size`, and `final_disk_size` act somewhat like private variables. This can be useful, because it stops users from accidentally overwriting them with incorrect values. I wrote [a full explaination of WDL variables elsewhere in this document](#variables-in-wdl-explanation-and-examples), but if it's these weird not-input-not-command, kind-of-private variables in particular you care about, you may want to skip to [the "private task-level variables" section](#private-task-level-variables).

### What about optional files?
If one of the files you want to account for is optional (ie, has ? in its type declaration), then ceil(size()) on that file will return an Int? instead of an Int. This can cause several issues, so it's best to use select_first() so you can still end up with an Int one way or another.

For example, let's say `some_other_file` was type File? instead of File. If `some_other_file` is defined, we want to set `other_size` to `ceil(size(some_other_file, "GB"))`, but if `some_other_file` is not defined, we want `other_size` to be zero. We can use select_first() for this, ending up with:

`Int other_size = select_first([ceil(size(some_other_file, "GB")), 0])`

[Click here](#how-to-use-optional-and-default-inputs-with-select_first) for more information on select_first().

## How to loop through a WDL array in a task's for loop
*See also: [How to run an entire WDL task on the contents of a WDL array](#how-to-use-scatter-to-do-a-wdl-task-on-every-object-in-an-array), which also acts a bit like a for loop*

Referencing a WDL array in a task's command section is a common use case, but isn't very intutive. When called in a task's command section, a WDL array must be referenced using the `sep` keyword. This keyword tells your WDL executor what character should seperate each value.

Take a look at [Patrick Magee](https://github.com/patmagee)'s example on [OpenWDL issue #500](https://github.com/openwdl/wdl/issues/500).

<!---https://github.com/openwdl/wdl/issues/500#issuecomment-1079292129--->
<!---also checked in grabbing_outputs.wdl--->
```
task A {
    input {
        Array[String] test = ["a","b","c","d"]
    }

    command {
       for foo in ~{sep=" " test}
       do 
           echo $foo
       done
    }
    output {
        Array[String] out = read_lines(stdout())
    }
}
```

The input variable `test` is being referenced in bash. Thanks to `sep=" "` we know that each value of test is going to be seperated with a single space. So, the command section essentially translates to this:

```
for foo in a b c d
	do
	    echo $foo
	done
```

...where `$foo` changes every iteration of the for loop. This isn't the only way to do this; I myself have written a lot of this kind of code, where I create a bash variable first, then loop through that bash variable. (In this example, `input_gds_files` is an input variable with type Array[File].)

<!---from assoc-aggregate.wdl in UWGAC WDL collection--->
```
GDS_FILES=(~{sep=" " input_gds_files})
	for GDS_FILE in ${GDS_FILES[@]};
	do
		cp ${GDS_FILE} .
	done
```


## How to use `scatter` to do a WDL task on every object in an array
*To loop within a single task's command section, see [this recipe](https://github.com/aofarrel/training-resources/tree/wdl-library/WDL#how-to-loop-through-a-wdl-array-in-a-tasks-for-loop) instead.*
* "I want to do something on every file in an array."
* "I want something like a for loop."
* "I want to parallelize a process."
* "I want to create multiple instances of a WDL task."

See also: https://support.terra.bio/hc/en-us/articles/360037128572-Scatter-Gather-Parallelism 

WDL does not have loops in the sense that other programming languages do. If you want to perform a task on every file within a group of files, it is recommended to use `scatter`. This can be quite helpful if you are WDLizing something that normally only takes in one file at a time.

Here is an example of a task that expects a single bam file and a reference genome.

<!-- corresponds to simple_scatter.wdl-->
```
version 1.0

task echo_one_bam {
	input {
		File inputBam
		File refGenome
	}
	command <<<
		echo ~{inputBam} > somefile.txt
		echo ~{refGenome} >> somefile.txt
	>>>

	output {
		String filenames = read_string("somefile.txt")
	}
}
```

But we want to be able to input an array of files into this workflow, not just one at a time. So, when writing the workflow portion of the WDL, `scatter` can be used to call just one file from an array of inputs.

<!-- corresponds to simple_scatter.wdl-->
```
workflow simple_scatter {
	input {
		Array[File] inputBams
		File refGenome
	}

	scatter(oneBam in inputBams) {
		call echo_one_bam as scattered_echo_one_bam { 
			input:
				inputBam = oneBam,
				refGenome = refGenome
		}
	}
}
```

Additionally, refGenome is also present. It is not being scattered upon, so it is an input for every iteration. In other words, every iteration of echo_one_bam will receive a single File for inputBam (exactly which file changes every iteration), and a single File for refGenome (every iteration getting the same one).

One important way that this differs from your typical for loop is that `scatter` is parallized, while iteration via a for loop is not ([usually](https://stackoverflow.com/questions/29156704/what-is-a-parallel-for-loop-and-how-when-should-it-be-used#:~:text=The%20summing%20for%20loop%20can,such%20as%20separate%20CPU%20cores.)). For instance, let's say you had ten input files, but the second one was invalid in some way. If you were iterating through these files with a standard for loop in Python, then you would error out on that file and the remaining eight would not run. But using `scatter` in WDL is different -- all other instances of the task may complete. As such, `scatter` cannot replace for loops where order matters.


## How to pass a scattered task's output to a non-scattered task
It is relatively easy to pass a scattered task's output to a non-scattered one. Building off the previous recipe, we can call another task in much the same way we pass output from any other task. Note that `call report` is outside the context of scatter.

<!-- corresponds to simple_scatter.wdl-->
```
workflow simple_scatter {
	input {
		Array[File] inputBams
		File refGenome
	}

	scatter(oneBam in inputBams) {
		call echo_one_bam as scattered_echo_one_bam { 
			input:
				inputBam = oneBam,
				refGenome = refGenome
		}
	}

	call report {
		input:
			filenames = scattered_echo_one_bam.filenames
	}
}
```

As noted in the previous recipe, each instance of `echo_one_bam`returns a String. So, that means that the `report` task will be given an Array[String].


## How to use optional and default inputs with select_first
* "I want to create optional inputs with built-in defaults."
* "I want to fall back on a known value (or another variable) if a variable is not defined"

We can use select_first() for these purposes. But before we go any further, a word of caution: It is a very common mistake is to use select_first() with the wrong syntax. It should be written as an array...

`select_first([optional_variable, "foo"])`

...not as a function with two arguments, such as `select_first(optional_variable, "foo")`.

Now that we got that sorted, let's actually use it. As stated before, select_first() takes in a single array. Usually, that array has only two components, the first one being a variable, the second being a value. If and only if the first variable is not defined, select_first() will evaluate to the second thing in the array. If the variable is defined, then it will evaluate to that variable.

Let's take another look at [the scatter example from earlier on in this document](#using-the-scatter-function-to-perform-a-task-on-every-x-in-an-array), but add in another argument. This argument has the purpose of echoing the user's favorite animal.

<!-- corresponds to simple_scatter.wdl-->
```
workflow simple_scatter {
	input {
		Array[File] inputBams
		File refGenome
		String? favoriteAnimal
	}

	# Figure out which animal to use, fall back to dogs if none specified
	String animal = select_first([favoriteAnimal, "dog"])

	scatter(oneBam in inputBams) {
		call echo_one_bam as scattered_echo_one_bam { 
			input:
				inputBam = oneBam,
				refGenome = refGenome,
				animal = animal
		}
	}
}
```

Now, let's update our task to include `animal` as an input. Thanks to select_first(), `animal` will always be defined, even if `favoriteAnimal` is not. Because `animal` is always defined, we can set it as type String instead of the optional String? type.

<!-- corresponds to simple_scatter.wdl-->
```
task echo_one_bam {
	input {
		File inputBam
		File refGenome
		String animal
	}
	command <<<
		echo ~{inputBam} > somefile.txt
		echo ~{refGenome} >> somefile.txt
		echo ~{animal}
	>>>

	output {
		String filenames = read_string("somefile.txt")
	}
}
```

If the user does not define `favoriteAnimal`, then `favoriteAnimal` will be undefined and select_first() will evaluate to "dog", meaning that `animal` will evaluate to dog. If the user defines anything for `favoriteAnimal`, then whatever the user defined will be used. Beware, "bad" input will still take precedence; all that select_first() is looking at is whether or not something is defined!


## How to use if/else (conditionals) in WDL
WDL allows for limited conditional reasoning when it comes to setting variables. You can use this to set runtime attributes in tasks, such as this:

```
runtime {
	disk: if howBigShouldDiskSizeBe == "very big" then "local-disk 400000 SSD" else "local-disk 10 SSD"
}
```

You can also use if statements to optionally call certain tasks. In the example below, if the user sets check_gds to `true`, then the check_gds_files task will be called. If not, the check_gds_files task will not be called. Because the actual workflow output is derived from the vcf2gds task, which will always run regardless of what `check_gds` is set to, `vcf2gds.gds_output` will always be defined (provided vcf2gds has an output called `gds_output` of course), so the workflow level output will always be valid.

<!-- corresponds to vcf2gds_with_optional_check.wdl-->
```
version 1.0

workflow vcf2gds_with_optional_check {
	input {
		Array[File] vcf_files
		Boolean check_gds = false
	}

	scatter(vcf_file in vcf_files) {
		call vcf2gds {
			input:
				vcf = vcf_file
		}
	}

	if(check_gds) {
		scatter(gds in vcf2gds.gds_output) {
			call check_gds_files {
				input:
					gds = gds
			}
		}
	}

	output {
		Array[File] gdss = vcf2gds.gds_output
	}
}
```

(Side note: If we had wanted to check if check_gds were false instead of true, we would write `if(!check_gds)`. Use `!` to indicate NOT in WDL.)

What's the point of a workflow like this if the workflow-level output isn't influenced by check_gds_files? It doesn't need to be. If check_gds_files finds something wrong, it can simply throw an error, resulting in the workflow failing. More detailed results could be gleaned from log files from check_gds_files. Remember, if a workflow fails, task-level outputs are also saved.

This sort of if statement doesn't just have to be a boolean. In this example, the merge and check merged tasks are only called if more than one input file is put in by the user. The user does not directly set `num_gds_files`, but `num_gds_files` is calculated based on `gds_files`, which the user does set.

<!-- corresponds to ldpruning_with_two_optional_steps.wdl-->
```
workflow ldpruning_with_two_optional_steps {
	input {
		Array[File] gds_files
	}

	Int num_gds_files = length(gds_files)

	scatter(gds_file in gds_files) {
		call prune {
			input:
				gds_file = gds_file
		}
	}

	if (num_gds_files > 1) {
		call merge_gds {
			input:
				gdss = prune.pruned_output
		}
		
		scatter(pruned_gds in prune.pruned_output) {
			call check_merged_gds {
				input:
					gds_file = pruned_gds,
					merged_gds_file = merge_gds.merged_gds_output

			}
		}
	}
}
```

However, you cannot use else statements in this maybe-run-a-task-maybe-not manner. WDL lacks an "else" keyword for anything besides setting a variable's value. If you need if/else, you're going to have to create mutually exclusive if-statements like this.

```
	if(check_gds) {
		scatter(gds in vcf2gds.gds_output) {
			call check_gds_files {
				input:
					gds = gds
			}
		}
	}
	if(!check_gds) {
		scatter(gds in vcf2gds.gds_output) {
			call something_else {
				input:
					gds = gds
			}
		}
	}
```

But even though this workaround is sufficient for most use cases, keep in mind that WDL executors are unaware that mutually exclusive if statements are, well, mutually exclusive. Given that WDL executors try to prevent you from creating duplicated out, you generally shouldn't try to call the same task in two mutually exclusive if-blocks.

One final note: `!` is not the same as "is not defined." Check if a variable is defined using [WDL-builtin defined()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#boolean-defined), and use `!` to determine if something is true or false.

```
workflow PrepareForAlignment {
	input {
		File fastq
		File ref_genome
		File? ref_genome_index
        	Boolean skip_trimming = false  # set to true if not Illumina
	}
	
	if(!defined(ref_genome_index)) {
		call index_ref {
			input:
				ref = ref_genome
		}
	}

	if(!skip_trimming) {
		call trimmomatic {
			input:
				fastq = fastq
		}
	}
}
```

For further examples, [see Terra's documentation on conditionals](https://support.terra.bio/hc/en-us/articles/360037128512-Conditionals-if-else-).


## How to call a WDL with another WDL
There are times that you will want to call one WDL file from another. This is most commonly used in checker workflows, which call a workflow with known inputs and compare the outputs with known truth files. Usually, the easiest way to import a WDL into another WDL is to import the raw github URL (it must be the raw version) of the workflow you want to import. This import should go on the very top of your WDL, just under the version number.

```
import "https://raw.githubusercontent.com/DataBiosphere/topmed-workflows/1.32.0/aligner/u_of_michigan_aligner/u_of_michigan_aligner.wdl" as TopMed_aligner
```

Now that your WDL is imported, you have two options here. Either you can call the entire workflow, or you can call steps within the imported workflow.

As an example for calling the entire workflow, see the [TOPMed U of Michigan Aligner Checker WDL](https://dockstore.org/workflows/github.com/DataBiosphere/topmed-workflows/checker_UM_aligner_wdl:1.32.0?tab=files). The not-checker WDL that the checker WDL imports starts like this:


```
workflow TopMedAligner {
	[...]
}
```


And, as seen earlier, we imported the WDL as TopMed_aligner. Therefore, in the checker WDL, the aligner can be called like this:


```
call TopMed_aligner.TopMedAligner as aligner {
	[...]
}
```


Where `TopMed_aligner` refers to the name used in the import statement and `TopMedAligner` is the name of the workflow as given within that imported file. However, if you wanted to call the tasks instead, simply replace `TopMedAligner` with the name of the step you wish to call, such as `PreAlign`.

Keep in mind that whenever you call a WDL from another WDL, both of them must be of the [same version of WDL](#wdl-versions). Also keep in mind that it is generally bad security practice to link WDLs to other WDLs outside of the context of checker workflows.

# Lessons
## Runtime attributes
Runtime attributes are defined in the runtime section of a WDL section. The WDL 1.0 spec only defines two runtime attributes, `docker` and `memory`. All other runtime attributes are at the whims of the WDL executor you are using.

* [Cromwell runtime attributes](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/)
* [miniwdl runtime attributes](https://miniwdl.readthedocs.io/en/latest/runner_reference.html#task-runtime-attributes)
* [WDL 1.0 spec runtime attributes](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#runtime-section)

Regardless of which executor you are using, `docker` should be written as a string, and `memory` can be written as either a integer (which will be interpreted as bytes) or a string. All of the following are valid `memory` strings which will give you 2 GB worth of memory:
* memory: "2GB"
* memory: "2GiB"
* memory: "2 GB"
* memory: "2 GiB"
* memory: 2147483648

The `docker` attribute is a little bit pickier. It should be written the same way you would write a `docker pull` command. For example, if you want to use the latest ubuntu image from Docker Hub, you can set `docker: "ubuntu:latest"` while if you wanted [this quay.io copy of bamstats](https://quay.io/repository/collaboratory/dockstore-tool-bamstats), you would use `docker: docker pull quay.io/repository/collaboratory/dockstore-tool-bamstats` in your runtime section.

## Best practices for reproducibility
A user putting the same data into your workflow should get the same output as someone else running on the same data. You can ensure this by making sure that's what's actually happening is always the same, or at least, as similar as possible. Some general guidelines to do this:
* Do not allow users to specify a Docker image
	* If you need to do some sort of fork where "if data is X use Docker A, if data is Y use Docker B," consider writing two tasks, with each task differing only in name and what Docker image they are using. This will at least create a clear record as to which container was used, as the task name will be shown in logs. It's not a perfect solution, especially considering WDL lacks `if/else` logic, but if something goes wrong, this method may be a little easier to troubleshoot than simply printing the name of the Docker image being used to the logs.
* Do not import scripts from other places -- everything should be pre-packaged in your Docker image
	* This includes other WDLs!
* Do not be afraid of printing helpful information to stdout.

Note that there are exceptions to these guidelines! For example, checker workflows always need to import the workflow they are checking.

You can also read [Dockstore's Best Practices for Secure and FAIR Workflows](https://docs.dockstore.org/en/stable/advanced-topics/best-practices/best-practices-secure-fair-workflows.html) for more tips.

## Different versions of WDL
There are multiple different versions of WDL. **In general, most users will want to use WDL 1.0 formatting.** To support older WDLs, Cromwell (and by extension the Dockstore CLI) will accept valid WDLs that follow a draft specification. Versioning is performed in the first line of your WDL and is written in plaintext, such as

```
version 1.0
```

Per [the official WDL spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#versioning), this first-line versioning must be used if a WDL file is intended to be parsed as anything except the second version of the draft (draft-2) specification. This specification is done at the top of the file. It's good to get into the habit of versioning your WDLs as you will likely eventually want to use syntax not present in draft-2.

All WDL files within a given workflow must use the same version of WDL. If a WDL with "version 1.0" at the top (which will be parsed as 1.0) tries to call a WDL without an explicit version number (which will be parsed as draft-2), the process will fail even if the contents of the files otherwise should be compatible.


### Some functions have changed in in WDL 1.0
As you might expect, certain functions work differently or are removed altogether in different versions of WDL. One that is worth notice is sub(), as it was sometimes used in draft versions to coerce floats into integers. This is valid in draft-2, so if your WDL **does not** specify `version 1.0` at the top, the following would be a valid way to coerce the float `disk_size` into a integer:

```
disks: "local-disk " + sub(disk_size, "\\..*", "") + " HDD"
```

However, this is invalid in WDL 1.0 and, if you are using Cromwell/womtool, will not pass the syntax check. To convert floats into integers, it is better to use `ceil()` as shown in [this recipe](#base-cloud-resources-requested-upon-size-of-input-files).


### Cromwell lags behind the current WDL spec
According to its git history, the WDL 1.1 spec was finalized around February 2021. As of my writing this (mid-July 2022), to my knowledge, Cromwell does not support WDL 1.1 nor is there a public roadmap for it picking up WDL 1.1 support. If you are writing a WDL for Terra, **do not** use WDL 1.1 features. If you are writing a workflow that must use a WDL 1.1 feature, consider using a different WDL executor which does support WDL 1.1, or asking on Stack Overflow or elsewhere for advice on creating a WDL 1.0-compatible workaround.

## Variables in WDL: Explanation and Examples
A lot of issues that pop up when writing WDLs come down to the fact that a WDL involves many different sets of variables, and they are often written similarly. It is clearer to talk about this using chevron notation for task sections, so I'll discuss it in that context. (I recommend always using chevron notation anyway.) 

*Note: WDL does not use phrases like "private task-level input variable" or any of the other names I am about to introduce on their spec. The 1.0 spec does not even have any hits for "private" if you search the document. In other words, there are no official name for these different kinds of variables. I am giving them names in hopes of making them easier to understand.*

When using chevron notation for a command section, an input variable is written `~{likeThis}`. This is different from a bash variable, which is usually written `${likeThis}`. Let's take a look at an example.

```
version 1.0

task look_at_all_those_chickens {
    input {
        String chickens
    }

    command <<<
        chickenscopy="~{chickens} duck sounds"
        echo ${chickenscopy} >> out.txt
    >>>

    output {
        String out = read_string("out.txt")
    }
}

workflow variable_examples {
	input {
		String birds
	}

	call look_at_all_those_chickens {
		input:
			chickens = birds
	}
}
```

### Workflow-level inputs vs task-level inputs vs bash variables
When running the workflow, the user needs to provide an array of strings for the workflow-level input variable `birds`. This variable is called as `chickens` in the `look_at_all_those_chickens` task. By setting the value of `birds`, the user is indirectly setting the value of `chickens`, because `chickens` equals `birds`. I call `birds` a *workflow-level input variable* and `chickens` a *task-level input variable*.

In the `look_at_all_those_chickens` task's command section, we want to access the `chickens` variable. Because we are using chevron notation, we do so using a tilde and curly braces, resulting in `~{chickens}`. In that same line, we are creating a new variable, `chickenscopy`, to the value of `~{chicken}`. If you are using Cromwell, the task-level input variable `chickens` is defined before the command section of the task even executes, which you can see nested among Cromwell's output when you run this WDL, assuming `birds` is set to "goose noises":

```
[2022-07-26 13:08:12,80] [info] BackgroundConfigAsyncJobExecutionActor [27ca62d3variable_examples.look_at_all_those_chickens:NA:1]:
chickenscopy="goose noises"
echo ${chickenscopy} >> out.txt
```

Why is `~{chickens}` replaced with "goose noises" but not `${chickenscopy}`? Because this print is done before the task's command section executes, so `${chickenscopy}` has not actually been set yet.

### Variable scope pt 1
Now, let's talk about scope. `chickenscopy` is only defined in the command section of a task, and by default, the command section of a task is written in bash. Therefore, `chickenscopy` is a bash variable. To access this bash variable in the next line, we use a dollar sign and curly braces, resulting in `${chickenscopy}` as our syntax. This is visually distinct from `~{chickens}`, which was defined outside this bash section, but can be referenced inside of it, because it is a task-level input variable. In fact, we can reference `chickens` in the task's output section, not just the task's command section. That means that this would also be valid:

```
    output {
        String out = chickens
    }
```

#### Side note: WDL's syntax inconsistencies in referencing a variable
> "Hey, wait a minute! I have to reference `chickens` as `~{chickens}` inside a command section, but in the output section, I can just write `chickens`?!"

Yes. It's inconsistent, I know. But that's how it works.

### Variable scope pt 2
In the example above, `String out = chickens` is functionally equivalent to `String out = read_string("out.txt")` because out.txt is just `chickenscopy` and `chickenscopy` is just `chickens`. You might be wondering why even bother with the read_string() version at all. Well, let's say `chickens` and `chickenscopy` weren't actually equivalent. Let's say we do something in the command section to `chickenscopy` to make it something else, and we want our output to be `chickenscopy`.

```
task look_at_all_those_chickens {
    input {
        String chickens
    }

    command <<<
        chickenscopy="~{chickens} duck sounds"
        echo ${chickenscopy} >> out.txt
    >>>

    output {
        String out = read_string("out.txt")
    }
}
```

In this scenario, we have to use read_string() because we cannot access `${chickenscopy}` outside of this task's command section (and because `chickens`, which would access in the output section, doesn't equal what we actually want). That is to say, **a variable defined in a task's command section cannot be accessed outside that task's command section.** This can be quite a headache when trying to get outputs in WDL, which is why it is common to write outputs to a file and then leverage WDL's [read_string()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-read_stringstringfile) or [read_int()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#int-read_intstringfile) in a task's outputs section. That's what we're doing here. We cannot access `${chickenscopy}` directly, but we can access the contents of any file on the disk, so we can just create a file that has the contents of `${chickenscopy}`, then read that as a string. We set that as a *task-level output variable* called `out`.

### Private task-level variables
So far we have:
* Workflow-level input variable `birds`  
* Task-level input variable `chickens`  
* Bash variable named `chickenscopy`  
* Task-level output variable `out`  

There is still one more to talk about. Let's say that `chickens` was not actually a string, but a file with an extension, such as `goose noises.mp3`

We want the name of the file, without the extension. Let's also we're running it on a backend that requires we define how much disk space the task needs. When our backend is Google Cloud based, such as Terra, the disk space argument represents a maximum; if you underestimate, your workflow will run out of disk space and fail. With that in mind, we want to make sure the amount of disk size we put in the task's runtime argument scales with the size of our inputs. We don't know how big of a file the user is going to input, so we can set a variable to that size. At the same time, we don't the user to think that variable is something they have to manually define, so we want to make that variable private. We can do this by sneaking it in *outside* the input and command sections of the task. In this little secret section, we can also call the WDL-builtin [basename()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-basenamestring) to lop off anything before and after the string we're looking for. This is what we end up with:

```
version 1.0

task look_at_all_those_chickens {
    input {
        File chickens
    }
    
    Int disk_size = ceil(size(chickens))
    String base_chicken = basename(chickens, ".mp3")

    command <<<
        chickenscopy="~{base_chicken} duck sounds"
        echo ${chickenscopy} >> out.txt
    >>>

    runtime {
        disks: "local-disk " + disk_size + " HDD"
    }

    output {
        String out = read_string("out.txt")
    }
}

workflow variable_examples {
    input {
        File birds
    }

    call look_at_all_those_chickens {
        input:
            chickens = birds
    }
}
```

Both `disk_size` and `base_chicken` cannot be directly set by the user, but like task-level inputs, they can be used throughout the task. They are essentially *private task-level input variables*. Note that the WDL spec calls them ["non-input declarations,"](https://github.com/openwdl/wdl/blob/69bbcb2cba54346b757b21b5b60d5d270f759eaf/versions/1.0/SPEC.md#non-input-declarations) not the term I'm using here. This begs the question as to whether or not they are truly input variables, but it really depends on your point-of-view: They can be input to a task's command section, but are not input by the user.

`disk_size`is referenced in the runtime section as if it were an input variable. It does not use `$` or `~` or `{}` when referenced here. If it were referenced in the output section, the same concept would apply.

`base_chicken` is referenced in the command section as if were an input variable. Like an input variable, when referenced in the command section, it needs to be referenced with a tilde and curly braces, ie, `~{base_chicken}`. (It should be noted that using basename() in this way isn't the only way to remove the full path and file extension from `chickens`. You could also use various bash programs such as sed to remove the extension in the task's command section.)


# Quick tips
## Variables
### Reserved keywords (don't use these as variable names)
A non-exhaustive list:
* command
* output
* file
* task
* workflow

A name that contains any of these words plus additional characters is fine, such as `File output_file = "foo.txt"`

### Variables in filenames

There are two different ways that you should represent a filename with a variable.

In the command part of a WDL, if the variable is an input variable, then it should follow input variable syntax.

✅samtools index ~{inputBam}  
❌samtools index ${inputBam}  

In any other part of a WDL, such as in a task's output section, it can follow bash syntax, even if it is an input variable.

✅ File bamIndex = "${inputBam}.bai"  
❌ File bamIndex = "$inputBam.bai"  
❌ File bamIndex = "$(inputBam).bai"  
❌ File bamIndex = "inputBam.bai"  


### User-set inputs should be defined thrice
When it comes to inputs that the user directly passes in at the start of a workflow, it can be helpful to remember the "rule of three." Inputs should be defined as a workflow input and a task input, plus in the workflow call section, for a total of three.

```
task index {
	input { 
		File bamFile 
	} 
	command <<< do something >>>
}
workflow indexABam {
	input { 
		File bamFile 
	}
	call index { input: bamFile = bamFile } 
}
```

## Command section syntax
### Be careful with comments
Because command sections of a WDL can interpret bash commands, and bash commands make use of the # symbol, womtool (and possibily other parsers) can misinterpret comments as syntax. This usually only happens if there are special characters in the comment; alphanumerics should work fine.

This will work:

```
command <<<
    echo foo  # this is a valid comment 
>>>
```

This will fail womtool: 

```
command <<< 
    echo foo  # using <<<this syntax>>> for your command section is ~{very cool}!
>>>
```

### Chevron syntax
*Note: To use this kind of syntax, you must be using WDL 1.0 or later -- see [here](https://github.com/aofarrel/training-resources/blob/wdl-library/WDL/README.md#different-versions-of-wdl) for guidelines on WDL versions*  
The command section of a WDL task can contain valid bash syntax, which often contains {curly braces}. However, the command section of a WDL task can also start with those very braces. In some circumstances the parser has difficulty with this combination, which is why the WDL allows -- and in some cases encourages -- using alternative syntax for the command section. This alternative syntax looks like this:

<table>
  <tr>
   <td>standard syntax
   </td>
   <td>chevron syntax
   </td>
  </tr>
  <tr>
   <td><code>command {</code>
<p>
<code>}</code>
   </td>
   <td><code>command &lt;<&lt;</code>
<p>
<code>>>></code>
   </td>
  </tr>
</table>

A surprising number of common errors can be sidestepped by using chevron syntax, so it is worth trying should you have trouble with a command in WDL.


## Runtime attributes
### Allow users to easily change runtime attributes
A recipe in this document shows how to autoscale cloud storage size. You can use the same logic to allow the user to specify how much memory a task will use when run on the cloud, for instance, or whether or not preemptibles should be used if running on Google Cloud Compute specifically.

It's generally a good idea to give users at least some control over memory and disk size. Also consider giving users the ability to use preemptibles -- your users may want to perform a quick run on downsampled test data, which could be a good use case for preemptibles. Alternatively, your users may consider running your WDL on more data than you've considered, which could preclude preemptibles on a task you'd otherwise consider to run within a reasonable timeframe.


### Not all runtime attributes are relevant to all platforms
Running WDL locally will ignore a WDL's values for runtime attributes that only apply to the cloud. For example, when using Cromwell, CPU and memory are recognized when running on AWS, GCP, or HCP, but do nothing on other platforms. This means if you have an error in a cloud-platform-only attribute, it will go undetected on your local runs. However, there are a handful of runtime attributes which are valid for both local and all cloud runs, such as `docker`.


### Some runtime attributes act differently on different platforms
In addition to not all platforms sharing the same runtime attributes, in some cases the same attribute can mean different things on different platforms. For instance, when using Cromwell, GCP tends to interpret memory as a minimum, but most HCP systems default to interpreting it as a maximum.


### Have some wiggle room for storage
_Note: This section applies to Cromwell. miniwdl appears to handle disk space differently._

When running on any cloud system besides AWS, you can use the disks attribute to request a certain amount of storage. If you are scaling your request disk storage size to the size of your inputs, it's a good idea to add some wiggle room. This is especially important if you know that a file will be reprocessed in some way that creates intermediate files. If you are using `ceil()` [like suggested earlier](#base-cloud-resources-requested-upon-size-of-input-files), you will already be rounding up. But if you run into errors about there not being enough space left, it's worth adding a few extra gigs in your calculation. You may even want to make it a multiple of one of your inputs. For instance, we can adjust the formula from the recipe to make it a multiple of our input cram file. Usually, multiplication of an input like this isn't necessary, but it is worth trying if you are plagued by `no space left on device`.

```
Int finalDiskSize = refSize + indexSize + (2*thisAmSize)
```

This is not relevant to AWS, as AWS EBS autoscales to the necessary size. Storage also does not apply to local runs.


### If at first you don't succeed, set maxRetries
Both Cromwell and miniwdl support `maxRetries` as a runtime attribute. If this is set to a nonzero value, your task will attempt to re-run if it fails.


### Cromwell can handle preemptible interruptions for you
If you include the runtime attribute `preemptible` in your WDL, you can specify the maximum number of times Cromwell will request a [preemptible machine](https://cloud.google.com/compute/docs/instances/preemptible) from Google for a task before defaulting back to a non-preemptible machine. For instance, if your set  `preemptible: 2`, your workflow will attempt a preemptible at first, and if that machine gets preempted, it will try again with a preemptible again, and if that second try is preempted, then it will use a non-preemptible.

Preemptibles are counted separately from `maxRetries`, assuming the failure while running on preemptibles is caused by the machine being preempted.


### Some runtime attributes must use integers and/or strings
When running on Google Cloud, the `disk` and `memory` runtime attributes must contain numbers of type int, not float. This is a limitation imposed by Google, not WDL executors.

However, disk and memory are somewhat unique -- although they must contain integers (at least on GCP), their overall format actually needs to be a string.

In the case of size, it must be a space-separated string containing three things:

1. Either the mount point as an absolute path
2. An integer representing disk space, in GB
3. Disk type, which can be either LOCAL, SSD, or HDD.

The [memory runtime attribute](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#memory) is a bit more flexible. While it still must be a string containing an integer, it does not need to be space separated. The following are all valid entries for memory:

* memory: "4G"
* memory: "4 G"
* memory: "4 GB"
* memory: "4000 MB"


## Troubleshooting
### Use pipefail to catch errors early
Towards the beginning of a command section within a task, it's good practice to include the following:

```
set -eux -o pipefail
```

Among [other things](https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/), this will make sure that your pipeline fails whenever a task returns something that isn't 0. This can be helpful if you have a long task that you don't wish to continue should something fail upstream. It can also catch more rare situations where a failure does not return 1, such as if you are running something in Golang. Remembering to `set -eux -o pipefail` will prevent errors from escaping your notice, or falsely seeming to succeed only to cause other errors later down the line.

### Know the quirks of your workflows
The more you know about what you are WDLizing, the easier and more foolproof the process will be. For instance, if you know what you are trying to WDLize will reindex an input file (such as cram) if you do not also include an index file (such as crai), you will likely want to account for slightly more disk space to allow for that indexing to occur. If, on the other hand, you know that your tool will fail later on down the line if no index file is provided, then you can ensure that the user includes the index by making them non-optional, allowing the workflow to exit quickly rather than fail after spending time and money on previous steps.


### Keep the purpose and scope of WDL in mind
If you are new to writing workflows, it is worth keeping in mind that **WDL is designed to run a series of commands, not be a stand-alone programming language in its own right**. If you are struggling to do something in WDL, it is worth taking a step back and thinking if it can be done with its own tasks. For instance, you may find it easier to parse your output as a task in WDL that calls upon a parsing script rather than trying to manipulate outputs within a workflow description section.


### Scattered tasks do not share allocated resources
Every instance of a scattered task is given the resources allocated to it in the task section of your WDL.

Let's say you are running an analysis on GCP, which makes use of the `memory` runtime attribute, and as an input you pass in an array of three files. You run a scattered task on this array, creating three instances of that task. In your WDL, you gave that task a runtime attribute of 15 GB of memory. Every instance of that task will be given 15 GB of memory, rather than each instance being allocated 5 GB of memory.


### Get familiar with womtool
womtool is Cromwell's built-in syntax validator, but it is also capable of generating a "skeleton" JSON file for a given WDL file, which you can use to quickly create input JSONs for your WDL file. You can read about that and its other uses on [Cromwell's documentation page](https://cromwell.readthedocs.io/en/stable/WOMtool/).

You can download womtool as its own jar file from [the Cromwell release page](https://github.com/broadinstitute/cromwell/releases/). In addition to the features just mentioned, the jar file can also be helpful for validating several WDL files rapidly without having to run them all on Cromwell, as it is the same tool used by Cromwell to validate syntax. When running WDL on the Dockstore CLI, womtool is also involved as the Dockstore CLI calls Cromwell, which contains womtool. Be aware that womtool's validation is checking for valid syntax -- as with all syntax validators it's still possible to have nonfunctional code that is still syntactically correct.


### Try a new WDL executor
Cromwell is what's used by Terra, but it isn't the only executor out there. If you would like something Python-based and/or less verbose, you may want to try [miniwdl](https://github.com/chanzuckerberg/miniwdl). Another Python-based executor is [Toil](https://toil.readthedocs.io/en/latest/running/wdl.html#wdl), although it should be noted its support for WDL is in alpha.


## Using Terra
### Use Firecloud API to help you debug
#### Get a full error traceback
You can use the following part of the Firecloud API to get a huge amount of information about your run. This will include all error codes associated with your WDL, which can be helpful, as Terra's current (it is July 2022 as I write this) UI does not show the full traceback when an error occurs.

https://api.firecloud.org/#/CromIAM%20Workflows%20(for%20Job%20Manager)/get_api_workflows__version___id__metadata


#### Download a WDL used on a Terra run to your local machine
_Note: While this has been made somewhat obsolete by a change in Terra's UI, it may still be useful to power users who wish to check multiple submission's WDLs quickly, or if you wish to recover a WDL from a Terra run (such as if you deleted your local copy)._

If you are developing a workflow and need to run multiple tests on Terra, you'll probably be updating your workflow a lot. When you go to run a workflow, you will be able to select the version -- release number or branches if imported from Dockstore, or snapshot if imported from the Broads Methods Repository. If you are running multiple versions of the same workflow, you might lose track of which run correlates to which WDL. While the WDL is now visible in the UI, if you prefer, you can extract the WDL once a workflow has finished using your local machine's command line.

When you click the "view" button to bring up the job manager, take note of the ID in the top, not to be confused with the workspace-id or submission-id.

![screenshot of Terra page to indicate where the ID is](images/terra_id.png)

You can use this ID on your local machine's command line to display the WDL on stdout.

```
curl -X GET "https://api.firecloud.org/api/workflows/v1/PUT-WORKFLOW-ID-HERE/metadata" -H "accept: application/json" -H "Authorization: Bearer $(gcloud auth print-access-token)" | jq -r '.submittedFiles.workflow'
```

Note that you will need to [install gcloud and login with the same Google account that your workspace uses](https://cloud.google.com/sdk/docs/quickstarts), and you will need jq to parse the result. jq can be easily installed on Mac with `brew install jq`

With this quick setup, you'll be able to check the WDL of previously run workflows, which can be helpful if you are running multiple versions of the same workflow to aid with debugging. To make this process more efficient, put a comment in the WDL itself explaining how each WDL differs.


### General data access issues on Terra
#### Using DRS URIs
DRS is a GA4GH standard providing a cloud-agnostic method to access data in the cloud. For NIH cloud platform users (BioData Catalyst, AnVIL, etc.), it is currently used to access data hosted by the Gen3 platform. When data is imported to Terra from Gen3, you will see that genomic files are accessed via "drs://" (rather than "gs://").

Cromwell in Terra will automatically resolve DRS URIs for you ([assuming your credentials are up-to-date](#make-sure-your-credentials-are-current)), so most WDLs will be able to use DRS URIs without any additional changes. 

However, depending on how your inputs are set up, some changes might be necessary, such as if you're using symlinks. When working with DRS URIs, sometimes you will want to have your inputs be considered strings rather than file paths.[This diff on GitHub](https://github.com/DataBiosphere/topmed-workflow-variant-calling/pull/4/files) shows the changes that were needed to make an already existing WDL work with DRS URIs on Terra. Although it is a somewhat complicated example, it may be a helpful template for your own changes.


#### Use gs rather than https inputs
If one of your inputs is in a Google bucket that you otherwise have access to (such as Google's own public genomics bucket), access it on Terra using its gs:// form rather than its storage.google.com form. You can just replace https://storage.google.com inputs with gs:// to do so; the remainder of the address will be the same. For instance,

```
https://storage.google.com/topmed_workflow_testing/topmed_aligner/reference_files/hg38/hs38DH.fa
```

becomes

```
gs://topmed_workflow_testing/topmed_aligner/reference_files/hg38/hs38DH.fa
```


#### Make sure your credentials are current
If you are having issues accessing controlled-access data on Terra, try refreshing your credentials. See Terra support on [linking Terra to external services](https://support.terra.bio/hc/en-us/articles/360038086332).


# Concluding Thoughts
If, when writing a WDL, you think to yourself "something about this workaround feels ridiculous," you are probably on the right track. Writing a WDL is not like writing in a standard programming language, because WDL is not designed to be a standard programming language.
